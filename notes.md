# Notes on Cloud Data Engineering

----
### Digital Badges
__

- List of credential earned:
    - <img src="img/notes-credentials.png" alt="" width="60%"/>
    - See: [Current Credentials](https://www.cloudskillsboost.google/public_profiles/70c3e8fe-77a0-45d9-a627-fa64f50ddafa)
..


----
### Google Cloud Functions
__

- Great Google Cloud Functions demo, 2017
    - https://www.youtube.com/watch?v=kXk78ihBpiQ
..


----
### DevOps
__ Overview

- Talk on DORA's Report 2019
    - https://www.youtube.com/watch?v=y0M9Z_zSmPo

- Why you should read DORA's 2019 Accelerate State of DevOps Report
    - https://www.youtube.com/watch?v=8M3WibXvC84
..


----
### Google Cloud Source Repositories
__ What is it

- Provide a managed Git version control remote
..


----
### Kubernetes
__ Kubernetes pods

- A Docker container is:
    - typically for a single microservice
    - a microservice typically talks to other microservices

- A Kubernetes pod is:
    - a group of containers tied together
    - with shared storage and network resources
    - to simplify admin and networking

..
__ Kubernetes Cluster Info

- View cluster configuration
    - `kubectl cluster-info`
    - `kubectl config view`

- View events and logs
    - `kubectl get events`
    - `kubectl logs POD_NAME`

..
__ Exposing Kubernetes pods

- Expose a deployment
    - `kubectl expose deployment hello-server --type=LoadBalancer --port 8080`
        - The cluster is exposed using a load-balancer
            - A Compute Engine load balancer [web](https://cloud.google.com/compute/docs/load-balancing/)

- Get public IP Address of service:
    - `kubectl get services`
        - CLUSTER-IP is internal IP of cluster
        - EXTERNAL-IP is public IP of cluster
..
__ Scale up a Kubernetes (K8s) cluster

- Scale up cluster to four (4) pods
    - `kubectl scale deployment $DEPLOYMENT --replicas=4`
        - Number of pods is now four (4) i.e. four containers
    - `kubectl get deployment`
        - View deployment to check number of pods
    - `kubectl get pods`
        - List pods. There should be four (4)
..
__ Update Deployments

- Steps
    - Update code
    - Build a docker image with updated tag
    - Push docker image to GCR
    - Update the deployment to use updated deployment
        - `kubectl edit deployment $DEPLOYMENT`
            - Update Spec > containers > image
..
__ References:

- kubectl commands
    - https://cloud.google.com/container-engine/docs/kubectl/

- A Compute Engine load balancer
    - https://cloud.google.com/compute/docs/load-balancing/

- Rolling Updates
    - https://cloud.google.com/container-engine/docs/rolling-updates

- Minikube, a simple setup of a single node kubernetes cluster for dev
    - http://kubernetes.io/docs/getting-started-guides/minikube/.

- Kubernetes Blog
    - http://blog.kubernetes.io/
..


----
### Docker
__ What is Docker, what's in it, how it helps:

- Docker is for:
    - separating application from infrastructure

- Docker combines:
    - kernel containerization features
    - workflows and
    - tooling

- Docker helps with:
    - testing
    - deployment
    - shorten time between writing code and running code

- Docker works with:
    - Kubernetes
..
__ Naming in Docker:

- Naming:
    - `Container ID` is a UUID generated by Docker e.g. 73bc1674ee1b
    - `Container Name` is human-readable string e.g. epic_ishizaka
    - `Image` is the docker image e.g. hello-world
    - `Command` is the image command to execute e.g. /hello or bash
..
__ Build a Docker image using a Dockerfile

- Sample dockerfile
    ```
        # Use an official Node runtime as the parent image
        FROM node:6

        # Set the working directory in the container to /app
        WORKDIR /app

        # Copy the current directory contents into the container at /app
        ADD . /app

        # Make the container's port 80 available to the outside world
        EXPOSE 80

        # Run app.js using node when the container launches
        CMD ["node", "app.js"]
    ```

- Notes:
    - All different apps such as NodeJS, etc may have parent Docker Images
        - This simplifies development
    - The /app dir must already exist in the NodeJS image
    - Keywords:
        - FROM, WORKDIR, ADD, EXPOSE, CMD
    - [Dockerfile command reference](https://docs.docker.com/engine/reference/builder/)

- Sample app.js file
    ```
        const http = require('http');
        const hostname = '0.0.0.0';
        const port = 80;
        const server = http.createServer((req, res) => {
            res.statusCode = 200;
              res.setHeader('Content-Type', 'text/plain');
                res.end('Hello World\n');
        });
        server.listen(port, hostname, () => {
            console.log('Server running at http://%s:%s/', hostname, port);
        });
        process.on('SIGINT', function() {
            console.log('Caught interrupt signal and will exit');
            process.exit();
        });
    ```

- Command to build the image (takes under a minute):
    - `docker build -t node-app:0.1 .
        - `-t`: names and tags an image with the name:tag syntax e.g.
          name is `node-app` and tag is `0.1`
    - Creates two docker images:
        - `node` which is the parent image
        - `node-app` which is based on `node
    - Both images are at about 884MB

- List of official node versions
    - https://hub.docker.com/_/node
..
__ Running and stopping a Docker image with port mapping

- Command:
    - `docker run -p 4000:80 --name my-app node-app:0.1`
        - `-p 4000:80` maps local port 4000 to container port 80
        - `--name my-app` specifies the container name
    - `docker run -d ...`
        - `-d` will run in background as daemon
    - `docker stop NAME`
        - stops a running container
    - `docker rm NAME`
        - removes execution history from `docker ps -a` output
..
__ Viewing and tailing outputs:

- Command:
    - `docker logs CONTAINER_ID`
        - tail the STDOUT of CMD
        - For CONTAINER_ID, just type enough to disambiguate the
          different container IDs
    - `docker logs -f CONTAINER_ID`
        - tail -f the STDOUT of CMD
..
__ Debugging containers

- Start an interactive Bash session inside a running container:
    - `docker exec -it CONTAINER_ID bash`
        - `-it` allows interaction with container
            - does this by allocating a pseudo-tty and keeping stdin open
        - bash will run in WORKDIR specified in the Dockerfile

- Inspect container metadata
    - `docker inspect CONTAINER_ID`
    - `docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' CONTAINER_ID`
        - Inspect specific fields
..
__ Publishing to Google Container Registry

- Naming images:
    - hostname for Google Container Registry (gcr) is `gcr.io`
    - project-id is the GCP Project-ID e.g. qwiklabs-gcp-00-98d3951b3b64
    - image is the image name e.g. node-app
    - tag is string of choice e.g. 0.1. Default to "latest" if unspecified

- Tag an image:
    - `docker tag node-app:0.2 gcr.io/$PROJECT_ID/node-app:0.2`
        - `tag` creates a tag, something like an alias

- Publish to GCR
    - `docker push gcr.io/$PROJECT_ID/node-app:0.2`
        - Note that the full name is specified
        - Apparently the target is something like a git repo?
..
__ Clean up Docker environment

- Stop and remove all containers
    - `docker stop $(docker ps -q)`
    - `docker rm $(docker ps -aq)`

- Remove all images
    - `docker rmi node-app:0.2 gcr.io/$PROJECT_ID/node-app node-app:0.1`
    - `docker rmi node:6`
    - `docker rmi $(docker images -aq) # remove remaining images`
    - `docker images`
..
__ Pull an image from gcr.io

- Pull an image:
    - `docker pull gcr.io/[project-id]/node-app:0.2`
        - fetch image from gcr.io
    - `docker run -p 4000:80 -d gcr.io/$PROJECT_ID/node-app:0.2`
        - run image in background
    - `curl http://localhost:4000`
        - Check output
..
__ References:

- Dockerfile command reference:
    - https://docs.docker.com/engine/reference/builder/#known-issues-run

- List of Official Node Images:
    - https://hub.docker.com/_/node

- Docker inspect reference:
    - https://docs.docker.com/engine/reference/commandline/inspect/#examples

- Docker exec reference:
    - https://docs.docker.com/engine/reference/commandline/exec/

- Docker run reference:
    - https://docs.docker.com/engine/reference/run/

- Google Container Registry:
    - https://cloud.google.com/container-registry/

- Specifying zones and buckets for GCR:
    - https://cloud.google.com/container-registry/docs/#pushing_to_the_registry
..

----
### Google Apps Script
__

- Server-side JS similar to Node.js, focused on tight integration with
  Workspace and other Google services
- To automate work across Google products and third party services
..


----
### Experimenting with Big Datasets
__

- To generate streaming data, one way is to simulate it. [github](https://github.com/GoogleCloudPlatform/training-data-analyst/courses/streaming/publish/send_sensor_data.py)
    - The notion of speedup factor in the code is interesting

- Sources of public datasets:
    - UCI ML Repository
        - Online Retail Data Set [uci](https://archive.ics.uci.edu/ml/datasets/online+retail)

- GCP:
    - data-to-insights ecommerce dataset [gcp](https://console.cloud.google.com/bigquery?project=data-to-insights&page=ecommerce)
..


----
### On Managing Change
__ Intro

- The notion that everything that we are doing is some form or other of
  managing and coping with change
    - This section of the notes try to note the changes and point to
      solutions
..
__ From Batch to Stream Processing

- The truth is business have always wanted stream processing however the
  computing technologies of the past have only been capable of batch
  processing.

..
__ From Flat Workloads to Seasonal Workloads

- Retailers, Movie industries have seasonal workloads
- Food on the other hand has rather flat workload
..


----
### Google Pub/Sub
__

- Pub/Sub
    - 100s of miliseconds latency
    - API: C#, Go, Java, Node, Python, Ruby
    - Save messages for 7 days
    - HIPAA compliance
    - Encrypted in-motion and at-rest
    - At least once semantics
    - Publisher: Pub/Sub Client that creates a Topic
        - Multiple, decoupled Publishers to the same topic
    - Subscriber: Pub/Sub Client that creates a Subscription (to a Topic)
        - A Subscription is only to a single topic
        - Muliple, decoupled Subscriptions to the same topic
    - Pub/Sub is essentially a decoupled enterprise message bus at scale
    - Pull subscriptions for manual pulls by any in a pool of workers
        - This is an interesting pattern

- Subscription
    - Can setup Filters on message attributes

- Patterns:
    - Pass-through:
        - 1 Publisher  -> 1 Topic -> 1 Subscription -> 1 Subscriber
    - Fan-in:
        - N Publishers -> 1 Topic -> 1 Subscription -> M Subscribers
        - All Publisher messages goes to the same Subscription
        - Each subscriber can filter on messages of interest
    - Fan-out:
        - 1 Publisher  -> 1 Topic -> 1 Subscription -> M Subscribers
            - All subscribers get all messages from Publisher
            - i.e. replicate publisher messages to all subscribers

- Push vs Pull deliveries
    - Pull Subscribers must send ACK after getting the Msg
    - Push Subscribers assumed to send ACK on a HTTP 200 OK status
        - Push Subscribers are web-hooks

- Message Replays
    - Allows Pub/Sub to send older messages to new subscribers

- Subcriber Worker Pools
    - Only one worker needs to send an ACK
    - A good pattern for autoscaling workers

- Messages
    - Limit: 10 MB
    - Can send binary data
    - Message attributes are stored separately as metadata to facilliate
      filtering to avoid deserializing

- Publishing
    - Send messages in batches
    - Pub/Sub waits till batch is full
    - Even with batch publishing, it is delivered one at a time to
      subscribers

- Subscribing
    - Message ordering supported if in the same region
    - Configured when creating a Subscription
    - Prevents Denial-of-Service on the push subscriber

- Subscription Delivery
    - Data is held/buffered until application is ready
    - Delivery failures should end-up in a Dead Letter Queue (DLQ)
    - Exponential Backoff implemented for delivery failures for better
      flow control
    - Data may be delivered out-of-order (not a queue)
    - Duplication will happen
    - Dataflow will dedup messages and also handle late data to mitigate
      out-of-order messages

- Write-ups
    - Differences between Google Pub/Sub and Apache Kafka, 2016
      [web](https://www.jesse-anderson.com/2016/07/apache-kafka-and-google-cloud-pubsub/)

..


----
### Dead Letter Queues (DLQ)
__

- A placeholder to DLQ
    - https://developer.confluent.io/learn-kafka/kafka-connect/error-handling-and-dead-letter-queues/

..


----
### Stream Processing
__

- Google Reference Architecture
    - Ingestion: Pub/Sub
    - Stream Processing:
        - Dataflow
            - (ELT|ETL: mostly aggregates & transforms)
            - Also enrichment
    - Storage: BigQuery, Cloud Bigtable
    - Analytics & Visualization: BigQuery

- Unbounded data
    - Characteristics:
        - Infinite dataset
        - Time of data element matters
        - Typically in-motion / held in temporaty storage
    - Contrast with bounded data:
        - Finite dataset
        - Time element usually disregarded
        - Typically at-rest / persisted in durable storage

- Stream Processing Systems
    - Low Latency
    - Speculative results
    - Controls for correctness
..
__ Use Cases

- Online/Real-time decisions (100 ms - 10 s)
    - Real-time recommendations
    - Fraud detection
    - Stock trading
    - ML applications

- Data Integration (10 sec - 10 min)
    - Real-time Data Warehouse Dashboards
    - ML applications:
        - Sentiment analysis
..
__ Challenge: Volume

- The challenge here is that the size of data has increased

- Google proposes autoscaling as a solution here
    - I think that's only if the data increase is seasonal or unpredictable
    - Pub/Sub can autoscale to handle changing volumes of data

..
__ Challenge: Velocity

- The challenge here is that time between each data element is shorter
    - Or more data elements in the same time span
    - Think of Black Friday or online sales days

- Google proposes streaming processing here
    - to handle variability of data arrivals
    - Dataflow handles streaming data
..
__ Challenge: Variety

- Voice data and images is one form of unstructured data

- Google proposes using AI to handle unstructured data

..
__ Solution Architectures

- Google Cloud Platform Solution
    - <img src="img/stream-processing-gcp-solution.png" alt="" width="80%"/>
..
__ Challenge: Late arriving data

- The challenge here is that data could arrive out of order
    - Google Pub/Sub does not guarantee ordering across regions
..
__ Windows in Stream Processing

- Motivation:
    - Aggregation functions typically occurs over a finite window
    - Streams are unbounded
    - Streams are divided into a series of finite windows
        - Think modulus, or the second hand of the clod
        - Making the infinite, finite

- Terms & Abbreviations:
    - DTS: Datetime Stamp

..


----
### Google Dataflow
__

- Dataflow for stream processing
    - Dedups by:
        - Maintaining a list of unique ids seen in the last 10 minutes
    - Supports streaming windows by:
        - Automatically windowing on date-time-stamp (DTS)
        - If there is a significant delay between the timestamp of an
          event and the time the message is actually received, able to
          use a PTransform to update the DTS in the message's metadata
          (which is used for windowing) to use the original event
          timestamp

..
__ Types of Windows

- 

..
__ Watermarks

- The watermark
- A nice talk on streaming processing, watermarks, 2016 [youtube](https://www.youtube.com/watch?v=TWxSLmkWPm4)
    - Motivates the need for stream processing and windows
    - Introduces the notion of low watermarks
    - Benefits of watermarks:
        - Visibility: Knows how much backlog there is
        - Actionability: Once passed the watermark, window is bounded,
          aggregates flushed
    - Formalizes notion of watermarks in a DAG pipeline
    - Inferring Source (PubSub) watermarks from metadata
    - Notion of data watermarks vs system watermarks
    - System watermarks to distinguish between data and system delays
    - Source matermarks are critical
..
__ Custom triggers

- Dataflow triggers window flush in several ways
    - AfterWatermark (Default)
        - Trigger after watermark
    - AfterProcessing
        - Trigger after processing for some time duration
    - AfterCount
        - Trigger after processing N elements

- Can be combined e.g.
    - While batching, trigger several releases using `AfterProcessing`
    - Then on watermark, trigger using `AfterWatermark`
    - And finally, trigger using `AfterCount` for late arriving elements
..


----
### Dimensions of a Solution
__

- Scalability
    - How does the solution handle larger input volumes

- Fault Tolerance
    - How does the solution handle component failures

- Latency
    - How long does it take the solution to convert input into output
..


----
### Change Data Capture
__

- Change data capture (CDC) provides historical change information for a
  user table by capturing both the fact that Data Manipulation Language
  (DML) changes (insert / update / delete) were made and the changed
  data. Changes are captured in real time by using a capture process
  that reads changes from the transaction log and places them in
  corresponding change tables. These change tables provide a historical
  view of the changes made over time to source tables. CDC functions
  enable the change data to be consumed easily and systematically.
    - From [microsoft](https://techcommunity.microsoft.com/t5/azure-sql-blog/stream-data-changes-from-a-cdc-enabled-azure-sql-database-to-an/ba-p/2570907)

- Me: It's making the database transaction log accessible to other
  processes.

..

----
### Good Blogs & Articles & Videos
__

- Stream Processing
    - A nice talk on streaming processing, watermarks, 2016 [youtube](https://www.youtube.com/watch?v=TWxSLmkWPm4)
    - Another talk on Apache Beam, 2016 [youtube](https://www.youtube.com/watch?v=E1k0B9LN46M)
    - Tyler Akidau on Data Processing at Google, 2016 [youtube](https://www.youtube.com/watch?v=9J_cWustI-A)

- Data Quality
    - Wikipedia on Data Quality [web](https://en.wikipedia.org/wiki/Data_quality)
    - Liliendahl on Data Quality [web](https://liliendahl.com/data-quality-3-0)
    - Has a good list of references at the bottom [web](https://profisee.com/data-quality-what-why-how-who/)

- Data Analytics
    - In Government: https://gcn.com/data-analytics/

- Reinforcement Learning
    - Google just gave control over data center cooling to an AI, 2018 [mit](https://www.technologyreview.com/2018/08/17/140987/google-just-gave-control-over-data-center-cooling-to-an-ai/)
    - Grandmaster level in StarCraft II using multi-agent reinforcement learning, 2019 [paper](https://storage.googleapis.com/deepmind-media/research/alphastar/AlphaStar_unformatted.pdf)
..


----
### Kubernetes
__

- Kubernetes
    - For deploying containerized applications
    - Runs on a cluster composed of a master node and worker nodes

- Kubernetes Deployment object
    - An object for deploying stateless applications like web servers

- Kubernetes Service object
    - Defined rules and load balancing for accessing applications from
      the internet
    - No Service means no access from internet
..


----
### Security Credentials
__

- How security credentials typically designed:
    - First, there are credentials, usually made up of a username and
      password
    - Credentials represent an identity
    - An identity may have one or more roles
    - These roles have access permissions
..


----
### Cloud Resources
__

- Naming
    - Use the format `team-resource` e.g. an instance `jupiter-webserver1`

- Compute sizes:
    - f1-micro for small Linux VMS
    - n1-standard-1 for Windows or other applications, such as Kubernetes nodes

..


----
### ETL Design Patterns
__

- Ingest data:
    - On success, move source data to "Archive"
    - On error, flag error, move source data to "Archive"

- Rolling Windows:
    - Make data windows an input parameter
    - Start and end date of the rolling window can be derived from
      job schedule date. Run dates are not suitable because re-runs will
      then not be idempotent.

- Workflow Scheduling
    - Also known as triggering jobs
    - It could be periodic (Pull)
        - This is named Pull cos periodically look into a folder then "Pull"
    - It could be event-driven (Push): e.g. triggered by new file in a folder
        - This is Push because file if Pushed to folder and that triggers

- Articles to research:
    - https://www.leapfrogbi.com/etl-design-patterns-the-foundation/
    - https://www.timmitchell.net/etl-best-practices/
    - https://aws.amazon.com/blogs/big-data/etl-and-elt-design-patterns-for-lake-house-architecture-using-amazon-redshift-part-1/
    - https://docs.microsoft.com/en-us/azure/architecture/data-guide/relational-data/etl
    - https://www.codeproject.com/Articles/5324207/Making-a-Simple-Data-Pipeline-Part-1-The-ETL-Patte
    - https://davewentzel.com/content/performant-etl-and-ssis-patterns/


..


----
### Google Cloud Composer
__

- Managed Apache Airflow as a Service

- What is it: An Orchestrator
    - Orchestrates: Google Cloud Services

- Contrast with: Google Cloud Data Fusion
    - Orchestrates: ETL Steps
..


----
### Google Cloud Functions
__

The function is written in Javascript. Mostly boilerplate code.

..


----
### GCP Billing
__

- Google Cloud Platform Billing is done by projects:
    - A Google Cloud project is an organizing entity for your Google
      Cloud resources. It often contains resources and services; for
      example, it may hold a pool of virtual machines, a set of
      databases, and a network that connects them together. Projects
      also contain settings and permissions, which specify security
      rules and who has access to what resources.
      [web](https://cloud.google.com/docs/overview/#projects)


..


----
### Apache Airflow
__

- Airflow [web](https://airflow.apache.org/)

- Executes tasks on an array of workers e.g.
    - Google Kubernetes engine workers

- DAG [web](https://airflow.apache.org/docs/apache-airflow/stable/concepts/dags.html)
    - A Directed Acyclic Graph is a collection of all the tasks you want
      to run, organized in a way that reflects their relationships and
      dependencies.

- Operator [web](https://airflow.apache.org/docs/apache-airflow/stable/concepts/operators.html)
    - The description of a single task, it is usually atomic. For
      example, the BashOperator is used to execute bash command.

- Task [web](https://airflow.apache.org/docs/apache-airflow/stable/concepts/tasks.html)
    - A parameterised instance of an Operator; a node in the DAG.

- Task Instance [web](https://airflow.apache.org/docs/apache-airflow/stable/concepts/tasks.html#task-instances)
    - A specific run of a task; characterized as: a DAG, a Task, and a
      point in time. It has an indicative state: running, success,
      failed, skipped, ...

- More concepts [web](https://airflow.apache.org/concepts.html#)

- Relies on 'Operators' to invoke services
    - See: https://airflow.apache.org/integration.html

- Airflow workflows are written in Python using 'Operators'
    - Parameterization can be done using macros

- Airflow visually represents the workflow in a Directed Acyclic Graph (DAG)

- Allows multi-cloud integration
    - Unlikely in practice due to egress charges

- Why AirFlow?
    - Visualization of workflow?
    - Visualization has job-completion-status indicators: Green|Pink|Red|etc
    - A simple dashboard of success|failed jobs
        - How to specify window?
    - Easily access logs of failed steps

- Support triggers via POST requests

- Auto retry N times supported
..


----
### Vertex AI
__

- Formerly called Cloud ML Engine

..


----
### Google Cloud Data Fusion
__

- Managed CDAP as a Service

- What is it: An Orchestrator
    - Orchestrates: ETL Steps

..


----
### Azure Data Explorer
__

- Source: Sent from Websites, Application, IoT

- Scales quickly
    - Uses Kusto Query Language, read-only
    - NoSQL under the hood

- Data Collection
    - IoT Hubs
    - Event Hubs
- Ingestion
- Storage
- Indexing structured and non-structured
- Querying
- Visualization: Power BI

- Workflow
    - Create storage: A cluster i.e. an instance of Azure Data Explorer
    - Create database: within cluster
    - Ingest data: loading data into the database
        - Setup automatic ingestion
        - Data is never deleted or updated
    - Query database
..


----
### Batch Data Processing
__

- Google Cloud Data Fusion handles only Batch Processing

..


----
### Cask, CDP, Cloud Data Fusion
__

- Curt Monash on Cask and CDAP 2015 [dbms2](http://www.dbms2.com/2015/03/05/cask-and-cdap/#more-9534)
..


----
### On Maven
__ Naming

- Maven projects are named using Maven UIDs:
    - groupID e.g. com.firexis.com
    - artifactID e.g. someApp
    - version e.g. 0.0.1-SNAPSHOT
..
__

- What is Maven:
    - A build tool that helps in project management
        - Generate source code
        - Generates docs
        - Compiles source code
        - Downloads libraries
        - Packages compiled codes into JAR files
        - Installing packaged code in local repo or central repo
    - Based on Project Object Model (POM)
        - An XML file that contains info regarding project and configuration
          details
        - Essential file is pom.xml
    - Used to build and manage Java-based project. Simplifies daya to
      day work of Java developers

- Maven solves the following problems:
    - Ensure needed JAR files are available
    - Downloads dependencies from mvnrepository.com
    - Creates the right project structure
    - Builds and deploys the project

- Understanding pom.xml
    - https://www.youtube.com/watch?v=KNGQ9JBQWhQ
    - A Project is defined uniquely by:
        - groupID
        - artifactID
        - version
    - Maven UIDs = groupID + artifactID + version
    - Depencies are also refered to via Maven UIDs
    - project
        - The root element in pom.xml
    - parent
        - to inherit from another pom.xml file
    - dependencies
        - container for dependency
    - dependency
        - a Maven UID that the current project depends on
    - build.plugins
        - container for plugins
    - build.plugins.plugin
        - A Maven UID of plugins needed during the build process

- Understanding Maven Archetypes
    - https://www.youtube.com/watch?v=U3AvNVT5j8w
    - A maven template built using Quarkus
        - A repeatedly usable archetype
        - Jackson for serailization
        - Panache for DB access
        - SmallRye OpenAI documentation
        - sonar-maven for Static Code analysis
        - pitest-junit5-plugin for mutation modification tests
..


---
### Google BigQuery
__ Naming

- BigQuery tables are named using:
    - Project ID e.g. qwiklabs-123
    - Dataset ID e.g. Marketing
    - Table Name e.g. Campaign
..
__ Links

- bq CLI [gcp](https://cloud.google.com/bigquery/docs/reference/bq-cli-reference)
- Standard SQL [gcp](https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax)
- Interactive SQL translator for Teradata SQL [gcp](https://cloud.google.com/bigquery/docs/interactive-sql-translator)
..

----
### Compare and contrast technologies
__

- Apache Spark vs Apache Beam
    - allegro.tech experience report, 2021 [web](https://blog.allegro.tech/2021/06/1-task-2-solutions-spark-or-beam.html)
        - A good write-up
        - Conclusion: Dataflow is more expensive, Dataproc is cheaper.
          So, essentially trading Dataflow cost for developer cost
    - A blog article circa 2016 [web](https://mpouttuclarke.wordpress.com/2016/02/06/apache-beam-vs-apache-spark-comparison/)
    - An SO question circa May 2017 [so](https://stackoverflow.com/questions/43581127)
..

----
### Notion of Component Gateway
__

- In Google Cloud Dataproc, there is the notion of a `Component
  Gateway`. It allows tools commonly associated with Apache Spark to be
  connected to the cluster.
- This is interesting in the sense that the backend setup and
  configuration is automated.
..


----
### List of Compatible OS, Hadoop, Spark
__

```
- Debian 10         , Hadoop 3.2, Spark 3.1
- RockyLinux 8      , Hadoop 3.2, Spark 3.1
- Ubuntu 18.04 LTS  , Hadoop 3.2, Spark 3.1

- CentOS 8          , Hadoop 2.10, Spark 2.4
- Debian 10         , Hadoop 2.10, Spark 2.4
- Ubuntu 18.04 LTS  , Hadoop 2.10, Spark 2.4

- Debian 10         , Hadoop 2.9, Spark 2.4
- Ubuntu 18.04 LTS  , Hadoop 2.9, Spark 2.4

- Debian 10         , Hadoop 2.9, Spark 2.3
- Ubuntu 18.04 LTS  , Hadoop 2.9, Spark 2.3
```


..


----
### Other Quick Bites
__

- Hadoop Cluster Size Calculation [web](https://www.youtube.com/watch?v=0ni6i8XkG88)
    - Key items:
        - Est. Overhead: 20% per node
        - Replication Factor contributes 3X
        - Intermediate contributes 1X
        - Est. Compression Factor: 2.3X
        - Est. Utilization Rate: 65%
        - Est. Data Growth Rate
- It's amazing what you can quickly learn online

- Spark Tutorial
    - A great Spark 2.0 demo at Spark SUMMIT 2016 [web](https://www.youtube.com/watch?v=2Qj1b4TruKA)
    - Data Engineering on databricks Demo, 2021 [web](https://www.youtube.com/watch?v=6Q8qPZ7c1O0)
        - Databricks is at v3.4.7 in 2021
    - Contrasts Spark and Hadoop [web](https://www.youtube.com/watch?v=QaoJNXW6SQo)
        - Databricks is a private company by the core developers of Spark
        - RDD: Resilient Distributed Datasets

- Delta Lakes [web](https://delta.io/) | [azure](https://docs.microsoft.com/en-us/azure/databricks/delta/delta-intro)
    - Evolution of Data Lakes and Data Warehouses into Lakehouses

..


----
### Some books on Skillsoft
__

- Snowflake Security: Security Your Snowflake Data Cloud, 2021 [web](https://orange.hosting.lsoft.com/trk/click?ref=znwrbbrs9_6-2e46bx332778x036174&)

..


----
### Columnar Databases
__

- VLDB 2009 Tutorial on Column-Oriented Database Systems [web](http://nms.csail.mit.edu/~stavros/pubs/tutorial2009-column_stores.pdf) | [local](tutorial2009-column_stores.pdf)
    - Interesting slides on what goes on under the hood in columnar databases.

- Inside Capacitor, BigQuery's next generation columnar storage format, 2016 [web](https://cloud.google.com/blog/products/bigquery/inside-capacitor-bigquerys-next-generation-columnar-storage-format)
    - Covers the storage system used in BigQuery
    - Storage system is one major component, the other is the execution engine

- Azure has Columnstore Indexes within SQL Server, 2021 [web](https://web.archive.org/web/20211214211514/https://docs.microsoft.com/en-us/sql/relational-databases/indexes/get-started-with-columnstore-for-real-time-operational-analytics?view=sql-server-ver15)
..


----
### Fun, short intros to Google Cloud offerings
__

- https://github.com/priyankavergadia/google-cloud-4-words
- https://github.com/priyankavergadia/GCPSketchnote
- https://www.youtube.com/playlist?list=PLIivdWyY5sqIQ4_5PwyyXZVdsXr3wYhip
..

----
### Serverless vs Managed Service
__

- Managed Service
    - Means: No setup, no management e.g. backup, replication, scaling
    - Examples:
        - DataProc

- Serverless
    - Means: Managed-Service + Event-driven/API endpoints
    - Examples:
        - BigQuery
        - DataFlow
        - Pub/Sub
        - Cloud Storage
..

----
### On Pseudo Benefits
__

- Pseudo benefits are things that sound like benefits but may not be.
    - An example is the availability of many choices to do something
    - Choices introduces complexity and the need to manage that
      complexity
    - If one selects the simplest option in a bevy of choices and simply
      stick to that, then it may help simplify the entire architecture

- In contrast, the ability to auto-scale is a true benefit
    - But only if you have lots of compute/storage and don't know what
      to do with them
..

----
### On Data Lineage
__

- In a recent development project, one of the things I find nice-to-have
  is the ability to know the call dependency graph.
    - A call stack is the list of function calls leading to the current
      function call. A call dependency graph would be a DAG going
      backwards from the current function to all the functions that can
      possibly call this function.
    - Why is this useful? When there is a problem with a function, the
      function will get modified as part of the fix. Knowing the
      dependency graph is useful because I'll know who will may get
      impacted upstream.
    - Another use of the call dependency graph is to work backwards to
      find impacted services and only restart those services to use the
      updated version of the function.

- How is that related to data lineage?
    - It's analogous. When there is a problem with data downstream. I'd
      like to be able to trace back to how that problem data was
      processed upstream.

..

----
### Roles, Responsiblities, Tasks
__ Roles and Responsibilities

- Role: Data Engineer
- Responsibilities:
    - Analyze, Explore a dataset
    - Ingest
    - Design Schema
    - Physical Data Design
    - Design and Implement Batch Data Pipelines
    - Design and Implement Streaming Data Pipelines
    - Design and Implement Data Quality Transforms
    - Design and Implement Data Enrichment Transforms
    - Monitoring: Capacity, Performance
    - Modify: ML team wants more data
    - Data access and governance
    - Metadata management
        - Tool: Cloud Data Catalog

- Role: Data Warehouse Engineer
    - Responsiblities:
        - Develop batch data pipelines
        - Develop streaming data pipelines
        - Support adhoc queries, visualization, reporting
        - Support machine learning
        - Manage security and governance

- Role: DBA
    - Responsibilities:
        - Backup & Recovery
            - Design Snapshots for quick recoveries
        - Scaling
        - Managing replicas
        - Encryption
        - Security & Patches
        - Vertical Scaling
        - Horizontal scaling (replicas)
        - Replication and Failover

- Role: ETL Engineer
    - Responsibilities
        - Logging: Log metadata, monitor logs
        - Documentation
        - Develop Load Processes
            - Capture Load Process logs and metadata
    - References:
        - https://www.timmitchell.net/post/2020/12/21/etl-antipattern-ignore-the-logging
..
__ Responsibilities and Tasks

- Physical Data Design
    - Partition
    - Denormalization (1NF?)

- Metadata management
    - Data lineage management
        - Start with adding labels
            - A label is a key, value pair
            - To datasets, tables, views

- Design and Implement Data Quality Transforms (DQT)
    - Roles: Data Engineer
    - Guildeline: Implement DQTs only when poor data quality interferes
      with data analysis and leads to poor business decisions.
    - Types of DQ Issues:
        - Not Valid
            - Problem: Data does not conform to your business rules
            - Example: $7 dollar transaction for standard movie ticket
              price of $10
            - Examples: Out of range, Empty/Null fields
        - Not Accurate
            - Problem: Data does not conform to an objective true value
            - Examples: Lookup issues, Datasets do not exist
        - Not Complete
            - Problem: Unable to analyze full dataset
            - Example: Failed to create, save and store whole datasets
            - Causes: Corrupted Data
        - Not Consistent
            - Problem: Two different computations from data ought to be
              the same but are not, so unable to proceed because do not
              know which to trust
            - Duplicate records
        - Not Uniform
            - Problem: When values in the same column but different rows
              mean different things
            - Example: Value of 10 in one row has the unit meters and the
              other has the unit of feett
    - Common DQ Problems:
        - Invalid data
            - Tools: Filter rows/aggregates invalid data
        - Duplication
        - Uniform
            - SQL CAST()
            - SQL FORMAT() [web](https://cloud.google.com/bigquery/docs/reference/standard-sql/string_functions#format_string)
    - Common DQ Tasks:
        - Cleanse
        - Match
        - De-dupe
        - Standardize

- Design and Implement Batch Data Pipelines
    - Roles: Data Engineer, Data Warehouse Engineer
    - Choose EL, ELT or ETL
        - Key consideration is how much transformation is needed
    - Design pipeline
    - Design execution triggers
        - Google Tools: Cloud Composer, Cloud Functions, Scheduled queries

- Analyze, Explore a dataset (Data Engineer)
    - Use SQL to quickly analyze, explore a dataset
        - How many tables
        - How many columns
        - Any columns partitioned or clustered
        - What's the data size
        - When was table last updated
        - Unique values per column
        - Null values per column

- Detect and handle late-arriving data
    - Data backfills

- Manage Security (Data Warehouse Engineer)
    - Encryption
    - Authentication
        - Configure accounts
    - Access control
        - Setup roles
            - Configure Read Permissions
            - Configure Write/Update Permissions
        - Assign access to roles
        - Row-level access policy
    - Audit logs (immutable logging)
        - Admin activity, System event, Data access
    - Log Monitoring

- Data access and governance
    - Define roles and authorization
    - Setup IAM roles and permissions
    - Setup Row-level security
    - Setup Authorized Views
    - Setup Access to Dataset, Columns, Rows
    - Restrict columns
    - Sharing Datasets

- Logging (ETL Engineer)
    - Things to log: Admin activity, System Event, Data Access
    - Logging needs to be [immutable](https://towardsdatascience.com/data-mesh-patterns-immutable-change-audit-log-aec93da33648)

- Design Schema (Data Engineer)
    - 

..
__ Case Studies

- You are building data pipelines. Some steps require call an API such
  as the Translate APi to augment/transform data. Another part of the
  pipeline requires processing more suited for procedural programming.
  What do you do:
    - Since the transformations cannot be expressed in SQL or are too
      complex to do in SQL, these transformations should be done in an
      ETL fashion.
    - Build an ETL pipeline
    - Implement the transforms in Dataflow, Dataproc, or use Data Fusion
      GUI to build pipelines

- There are some gaps in historical data. What are you going to do?
    - Discuss with the business users
    - Backfill with augmented data with the same distribution generated
      from the previous three years of data

- Need to batch load historical data. Data is clean and correct:
    - Since data is clean and correct, no further transformation needed
    - Use EL data pipeline
        - File hits Cloud Storage
        - Cloud Function invokes BigQuery load
        - Table appended to
    - To expose loaded file for use:
        - Table stored in private dataset
        - Public view that performs integrity checks
        - SQL query transforms and creates destination public table

- You create an image service. Users upload lots of images. The Vision
  API is used to identify objects in the image. How would you store this
  data for unknown future analysis?
    - The Vision API returns JSON data.
    - Since future use cases is unknown, the raw JSON data and a pointer
      to the original image should be stored.
    - This is an ELT pipeline
    - EL the image and JSON
    - Later, transforms may be applied to query the JSON to extract
      required data
..


----
### Lab: Vertex AI: Predicting Loan Risk with AutoML
__

Sun, 20 Mar 2022 (2h30m / 5 Credits)
- Course: [Google Cloud Big Data and Machine Learning Fundamentals](https://www.cloudskillsboost.google/course_templates/3)

- Goals:
    - Use AutoML to build a machine learning model to predict loan risk
    - 2,050 data points from financial institution
    - AutoML requires minimum 1,000 data points

- Tasks:
    - Task 1: Prepare the training data
    - Task 2: Train your model
    - Task 3: Evaluate the model performance 
    - Task 4: Deploy the model
    - Task 5: Shared Machine Learning (SML) Bearer Token
    - Task 6: Get predictions

- Notes:
    - [Vertex Pricing](https://cloud.google.com/vertex-ai/pricing?_ga=2.181173863.-1790764198.1647754986)
    - [Vertex AI documentation](https://cloud.google.com/ai-platform-unified/docs)
    - [Google Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)

- Task 1: Prepare the training data
    - Navigation menu > Vertex AI
    - Create dataset name: LoanRisk
    - Upload data options:
        - Upload a local file from your computer.
        - Select files from Cloud Storage.
        - Select data from BigQuery.
    - Import from Cloud Storage:
        - spls/cbl455/loan_risk.csv
    - Generate statistics
        - To see statistics of loans data
            - Total rows: 2,050
            - Age: 2048 distinct values
            - ClientID: 2,050 distinct values
            - Default: 2 distinct values
            - Income: 2050 distinct values
            - Loan: 2050 distinct values

- Task 2: Train your model
    - Strangely, the dataset name expected is "LoanRisk"
    - Set Objective = Classification
    - Model Details:
        - Model name: LoanRisk
        - Target column: Default
        - Advanced Model Options:
            - To determine training vs testing data & setup encryption
    - Training options
        - Specify columns (features) to include in training model
        - Exclude: ClientID
        - Advanced Training Options:
            - Can select different optimization objectives
            - See: https://cloud.google.com/vertex-ai/docs/training/tabular-opt-obj
    - Compute & Pricing
        - Set Budget: 1 (node hours)
            - 1 compute hour is a good start
        - Early Stopping: Enabled
        - Click: Start training

- Task 3: Evaluate the model performance (demo only)
    - This task is demo-only, no real hands-on cos the training takes an hour
    - On [ROC Curve and AUC](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)
    - Navigate > Vertex AI > Models
    - Click on model trained > Evaluate
    - Possible to manually adjust the confidence threshold
        - <img src="img/notes-confidence-threshold.png" alt="" width="80%"/>
        - In image above, looking at the Precision-recall by threhold
          graph on the right, I'd raise the threshold to about 0.72
          because higher values result in lower recall. But the actual
          change will depend on the business case

- Task 4: Deploy the model (demo only)
    - This task is demo-only, no real hands-on cos the training takes an hour
    - Create and define an endpoint
        - Navigate to model page > Deploy and test
        - Click: Deploy to endpoint
        - Endpoint name: LoanRisk
        - Click: Continue
    - Model settings and monitoring
        - Traffic splitting settings: Unchanged
        - Machine type: n1-standard-8, 8 vCPUs, 30 GiB memory
        - All other settings: Unchanged
        - Click: Deploy

- Task 5: Shared Machine Learning (SML) Bearer Token
    - This is to get a token to use the ML model for prediction
    - Prepare Task 6 before doing the below:
        - Log in to https://gsp-auth-kjyo252taq-uc.a.run.app/
        - Copy token to clipboard
            - Token is available for about 60secs.

- Task 6: Get predictions
    - Setup environment variables needed:
    - AUTH_TOKEN: To access the endpoint
    - ENDPOINT: The ML predict endpoint
    - INPUT_DATA_FILE: Input file containing features as JSON

```
# Download files used
gsutil cp gs://spls/cbl455/cbl455.tar.gz .
tar -xvf cbl455.tar.gz

# Setup environment variables:
AUTH_TOKEN="INSERT_SML_BEARER_TOKEN"
ENDPOINT="https://sml-api-vertex-kjyo252taq-uc.a.run.app/vertex/predict/tabular_classification"
INPUT_DATA_FILE="INPUT-JSON"

# Make request using smlproxy
# smlproxy is a binary executable to communicate with the backend
./smlproxy tabular \
  -a $AUTH_TOKEN \
  -e $ENDPOINT \
  -d $INPUT_DATA_FILE
```
..


----
### Confusion Matrix, Recall and Precision
__

- Confusion Matrix defines the following:
    - True Positive   TP   Predicted üêà Cat / Turned out to be üêà Cat   ‚úîÔ∏è
    - True Negative   TN   Predicted üêï Dog / Turned out to be üêï Dog   ‚úîÔ∏è
    - False Positive  FP   Predicted üêà Cat / Turned out to be üêï Dog   ‚ùå
    - False Negative  FP   Predicted üêï Dog / Turned out to be üêà Cat   ‚ùå

- Helps to think of the confusion matrix as follows:
    - Predictions come first, the actual values come later and compared
      to prediction
    - We are typically interested in Positive predictions
    - Hence the Recall and Precision are useful metrics
    - Negative predictions can easily be turned into positive
      predictions

- Recall almost 1     ‚áí  Very few false negatives
    - Missed some which were actually positive, hence imperfect recall
    - Like having to recall a list of 16 items, and of the 16 items
      remembered, some were not in the original list. Imperfect recall.
    - TP / (TP + FN)
    - Question: Did you get all of them right?

- Precision almost 1  ‚áí  Very few false positives
    - Lots of positive predictions but some turned out to be false
    - Like shooting at real and fake ducks, some ducks turned out to be
      fake, hence imperfect precision
    - TP / (TP + FP)
    - Of all the ones you shot, how many were what you aimed for?
..


----
### First Class Objects
__

- Google Feature Store
    - The Feature Store in Vertex AI has the result of making features
      [first class
      objects](https://www.cs.iusb.edu/~danav/teach/c311/c311_2_firstclass.html)
      in AI. This is just fancy speak for making features the priority.
    - And making it a priority means make it easy to work with, to
      organize, to store, to aggregate, to share, to reuse, etc.
    - This leads to the interesting question:
        - What are the priority objects in different stages of the Data
          Engineering workflow?

- Data Warehouses
    - The first class objects in Data Warehouses should to be schema?
    - Schema should be easy to work with, to organize, to manage
    - Schema should be fluid, composable and all the nice buzzwords
    - Maybe..

..


----
### Lab: [Predicting Visitor Purchases with a Classification Model with BigQuery ML](https://www.cloudskillsboost.google/course_sessions/887252/labs/198828)
__

Sat, 19 Mar 2022 (1hr20m 5 Credits)

- Tasks:
    - Task 1. Explore ecommerce data
    - Task 2. Select features and create your training dataset
    - Task 3. Create a BigQuery dataset to store models
    - Task 4. Select a BigQuery ML model type and specify options
    - Task 5. Evaluate classification model performance
    - Task 6. Improve model performance with feature engineering
    - Task 7. Predict which new visitors will come back and purchase

- Notes:
    - [Course dataset](https://console.cloud.google.com/bigquery?p=data-to-insights&d=ecommerce&t=web_analytics&page=table)
    - [Field Definitions](https://support.google.com/analytics/answer/3437719?hl=en)
    - [Preview Demo Dataset](https://bigquery.cloud.google.com/table/data-to-insights:ecommerce.web_analytics?tab=preview)
    - Other Models:
        - [Deep Neural Networks](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-dnn-models)
        - [Boosted Decision Trees (XGBoost)](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-boosted-tree)
        - [AutoML Tables Models](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-automl)
        - [Importing Custom TensorFlow Models](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-tensorflow)

- Task 1. Explore ecommerce data
    - Data is already in BigQuery
    - Ask: Out of the total visitors who visited our website, what % made a purchase?

```
-- 1. What % of visitors made a purchase?

#standardSQL
WITH visitors AS(
SELECT
COUNT(DISTINCT fullVisitorId) AS total_visitors
FROM `data-to-insights.ecommerce.web_analytics`
),
purchasers AS(
SELECT
COUNT(DISTINCT fullVisitorId) AS total_purchasers
FROM `data-to-insights.ecommerce.web_analytics`
WHERE totals.transactions IS NOT NULL
)
SELECT
  total_visitors,
  total_purchasers,
  total_purchasers / total_visitors AS conversion_rate
FROM visitors, purchasers


-- 2. What are the top 5 selling products?
SELECT
  p.v2ProductName,
  p.v2ProductCategory,
  SUM(p.productQuantity) AS units_sold,
  ROUND(SUM(p.localProductRevenue/1000000),2) AS revenue
FROM `data-to-insights.ecommerce.web_analytics`,
UNNEST(hits) AS h,
UNNEST(h.product) AS p
GROUP BY 1, 2
ORDER BY revenue DESC
LIMIT 5;

-- 3. How many visitors bought on subsequent visits to the website?
# visitors who bought on a return visit (could have bought on first as well
WITH all_visitor_stats AS (
SELECT
  fullvisitorid, # 741,721 unique visitors
  IF(COUNTIF(totals.transactions > 0 AND totals.newVisits IS NULL) > 0, 1, 0) AS will_buy_on_return_visit
  FROM `data-to-insights.ecommerce.web_analytics`
  GROUP BY fullvisitorid
)
SELECT
  COUNT(DISTINCT fullvisitorid) AS total_visitors,
  will_buy_on_return_visit
FROM all_visitor_stats
GROUP BY will_buy_on_return_visit
```

- Task 2. Select features and create your training dataset

```
SELECT
  * EXCEPT(fullVisitorId)
FROM
  # features
  (SELECT
    fullVisitorId,
    IFNULL(totals.bounces, 0) AS bounces,
    IFNULL(totals.timeOnSite, 0) AS time_on_site
  FROM
    `data-to-insights.ecommerce.web_analytics`
  WHERE
    totals.newVisits = 1)
  JOIN
  (SELECT
    fullvisitorid,
    IF(COUNTIF(totals.transactions > 0 AND totals.newVisits IS NULL) > 0, 1, 0) AS will_buy_on_return_visit
  FROM
      `data-to-insights.ecommerce.web_analytics`
  GROUP BY fullvisitorid)
  USING (fullVisitorId)
ORDER BY time_on_site DESC
LIMIT 10;
```

- Task 3. Create a BigQuery dataset to store models
    - Need a dataset to store models
    - Dataset ID: ecommerce

- Task 4. Select a BigQuery ML model type and specify options
    - Create a logistic regression model
    - Notice the WHERE clause that splits data into training and evaluation
    - Training takes 5-10 minutes

```
CREATE OR REPLACE MODEL `ecommerce.classification_model`
OPTIONS
(
model_type='logistic_reg',
labels = ['will_buy_on_return_visit']
)
AS
#standardSQL
SELECT
  * EXCEPT(fullVisitorId)
FROM
  # features
  (SELECT
    fullVisitorId,
    IFNULL(totals.bounces, 0) AS bounces,
    IFNULL(totals.timeOnSite, 0) AS time_on_site
  FROM
    `data-to-insights.ecommerce.web_analytics`
  WHERE
    totals.newVisits = 1
    AND date BETWEEN '20160801' AND '20170430') # train on first 9 months
  JOIN
  (SELECT
    fullvisitorid,
    IF(COUNTIF(totals.transactions > 0 AND totals.newVisits IS NULL) > 0, 1, 0) AS will_buy_on_return_visit
  FROM
      `data-to-insights.ecommerce.web_analytics`
  GROUP BY fullvisitorid)
  USING (fullVisitorId)
;
```

- Task 5. Evaluate classification model performance
    - Use Receiver Operating Characteristics (ROC) to evaluate
    - Maximize are under the curve (AUC)
    - Notice the WHERE clause now evaluates on two (2) months of data

```
SELECT
  roc_auc,
  CASE
    WHEN roc_auc > .9 THEN 'good'
    WHEN roc_auc > .8 THEN 'fair'
    WHEN roc_auc > .7 THEN 'not great'
  ELSE 'poor' END AS model_quality
FROM
  ML.EVALUATE(MODEL ecommerce.classification_model,  (
SELECT
  * EXCEPT(fullVisitorId)
FROM
  # features
  (SELECT
    fullVisitorId,
    IFNULL(totals.bounces, 0) AS bounces,
    IFNULL(totals.timeOnSite, 0) AS time_on_site
  FROM
    `data-to-insights.ecommerce.web_analytics`
  WHERE
    totals.newVisits = 1
    AND date BETWEEN '20170501' AND '20170630') # eval on 2 months
  JOIN
  (SELECT
    fullvisitorid,
    IF(COUNTIF(totals.transactions > 0 AND totals.newVisits IS NULL) > 0, 1, 0) AS will_buy_on_return_visit
  FROM
      `data-to-insights.ecommerce.web_analytics`
  GROUP BY fullvisitorid)
  USING (fullVisitorId)
));
```

- Task 6. Improve model performance with feature engineering
    - Add more features:
        - How far the visitor got in the checkout process on their first visit
        - Where visitors came from (traffic source: organic search, referring site etc.)
        - Device category (mobile, tablet, desktop)
        - Geographic information (country)
    - UNNEST() function
        - To break apart ARRAYS into separate rows. Making the table in 1NF (First Normal Form)

```
# Create second model
CREATE OR REPLACE MODEL `ecommerce.classification_model_2`
OPTIONS
  (model_type='logistic_reg', labels = ['will_buy_on_return_visit']) AS
WITH all_visitor_stats AS (
SELECT
  fullvisitorid,
  IF(COUNTIF(totals.transactions > 0 AND totals.newVisits IS NULL) > 0, 1, 0) AS will_buy_on_return_visit
  FROM `data-to-insights.ecommerce.web_analytics`
  GROUP BY fullvisitorid
)
# add in new features
SELECT * EXCEPT(unique_session_id) FROM (
  SELECT
      CONCAT(fullvisitorid, CAST(visitId AS STRING)) AS unique_session_id,
      # labels
      will_buy_on_return_visit,
      MAX(CAST(h.eCommerceAction.action_type AS INT64)) AS latest_ecommerce_progress,
      # behavior on the site
      IFNULL(totals.bounces, 0) AS bounces,
      IFNULL(totals.timeOnSite, 0) AS time_on_site,
      totals.pageviews,
      # where the visitor came from
      trafficSource.source,
      trafficSource.medium,
      channelGrouping,
      # mobile or desktop
      device.deviceCategory,
      # geographic
      IFNULL(geoNetwork.country, "") AS country
  FROM `data-to-insights.ecommerce.web_analytics`,
     UNNEST(hits) AS h
    JOIN all_visitor_stats USING(fullvisitorid)
  WHERE 1=1
    # only predict for new visits
    AND totals.newVisits = 1
    AND date BETWEEN '20160801' AND '20170430' # train 9 months
  GROUP BY
  unique_session_id,
  will_buy_on_return_visit,
  bounces,
  time_on_site,
  totals.pageviews,
  trafficSource.source,
  trafficSource.medium,
  channelGrouping,
  device.deviceCategory,
  country
);


# Evaluate the second model
#standardSQL
SELECT
  roc_auc,
  CASE
    WHEN roc_auc > .9 THEN 'good'
    WHEN roc_auc > .8 THEN 'fair'
    WHEN roc_auc > .7 THEN 'not great'
  ELSE 'poor' END AS model_quality
FROM
  ML.EVALUATE(MODEL ecommerce.classification_model_2,  (
WITH all_visitor_stats AS (
SELECT
  fullvisitorid,
  IF(COUNTIF(totals.transactions > 0 AND totals.newVisits IS NULL) > 0, 1, 0) AS will_buy_on_return_visit
  FROM `data-to-insights.ecommerce.web_analytics`
  GROUP BY fullvisitorid
)
# add in new features
SELECT * EXCEPT(unique_session_id) FROM (
  SELECT
      CONCAT(fullvisitorid, CAST(visitId AS STRING)) AS unique_session_id,
      # labels
      will_buy_on_return_visit,
      MAX(CAST(h.eCommerceAction.action_type AS INT64)) AS latest_ecommerce_progress,
      # behavior on the site
      IFNULL(totals.bounces, 0) AS bounces,
      IFNULL(totals.timeOnSite, 0) AS time_on_site,
      totals.pageviews,
      # where the visitor came from
      trafficSource.source,
      trafficSource.medium,
      channelGrouping,
      # mobile or desktop
      device.deviceCategory,
      # geographic
      IFNULL(geoNetwork.country, "") AS country
  FROM `data-to-insights.ecommerce.web_analytics`,
     UNNEST(hits) AS h
    JOIN all_visitor_stats USING(fullvisitorid)
  WHERE 1=1
    # only predict for new visits
    AND totals.newVisits = 1
    AND date BETWEEN '20170501' AND '20170630' # eval 2 months
  GROUP BY
  unique_session_id,
  will_buy_on_return_visit,
  bounces,
  time_on_site,
  totals.pageviews,
  trafficSource.source,
  trafficSource.medium,
  channelGrouping,
  device.deviceCategory,
  country
)
));
```

- Task 7. Predict which new visitors will come back and purchase

```
SELECT
*
FROM
  ml.PREDICT(MODEL `ecommerce.classification_model_2`,
   (
WITH all_visitor_stats AS (
SELECT
  fullvisitorid,
  IF(COUNTIF(totals.transactions > 0 AND totals.newVisits IS NULL) > 0, 1, 0) AS will_buy_on_return_visit
  FROM `data-to-insights.ecommerce.web_analytics`
  GROUP BY fullvisitorid
)
  SELECT
      CONCAT(fullvisitorid, '-',CAST(visitId AS STRING)) AS unique_session_id,
      # labels
      will_buy_on_return_visit,
      MAX(CAST(h.eCommerceAction.action_type AS INT64)) AS latest_ecommerce_progress,
      # behavior on the site
      IFNULL(totals.bounces, 0) AS bounces,
      IFNULL(totals.timeOnSite, 0) AS time_on_site,
      totals.pageviews,
      # where the visitor came from
      trafficSource.source,
      trafficSource.medium,
      channelGrouping,
      # mobile or desktop
      device.deviceCategory,
      # geographic
      IFNULL(geoNetwork.country, "") AS country
  FROM `data-to-insights.ecommerce.web_analytics`,
     UNNEST(hits) AS h
    JOIN all_visitor_stats USING(fullvisitorid)
  WHERE
    # only predict for new visits
    totals.newVisits = 1
    AND date BETWEEN '20170701' AND '20170801' # test 1 month
  GROUP BY
  unique_session_id,
  will_buy_on_return_visit,
  bounces,
  time_on_site,
  totals.pageviews,
  trafficSource.source,
  trafficSource.medium,
  channelGrouping,
  device.deviceCategory,
  country
)
)
ORDER BY
  predicted_will_buy_on_return_visit DESC;
```

- Challenge:
    - Use the XGBoost Classifier:
        - Use options/hyperparameters:
            - L2_reg = 0.1
            - num_parallel_tree = 8
            - max_tree-depth = 10
    - Change only one line of SQL
    - See: [Boosted Decision Trees (XGBoost)](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-boosted-tree)

```
# 1. Train the model
CREATE OR REPLACE MODEL `ecommerce.classification_model_3`
        OPTIONS
          (model_type='BOOSTED_TREE_CLASSIFIER' , l2_reg = 0.1, num_parallel_tree = 8, max_tree_depth = 10,
              labels = ['will_buy_on_return_visit']) AS
        WITH all_visitor_stats AS (
        SELECT
          fullvisitorid,
          IF(COUNTIF(totals.transactions > 0 AND totals.newVisits IS NULL) > 0, 1, 0) AS will_buy_on_return_visit
          FROM `data-to-insights.ecommerce.web_analytics`
          GROUP BY fullvisitorid
        )
        # add in new features
        SELECT * EXCEPT(unique_session_id) FROM (
          SELECT
              CONCAT(fullvisitorid, CAST(visitId AS STRING)) AS unique_session_id,
              # labels
              will_buy_on_return_visit,
              MAX(CAST(h.eCommerceAction.action_type AS INT64)) AS latest_ecommerce_progress,
              # behavior on the site
              IFNULL(totals.bounces, 0) AS bounces,
              IFNULL(totals.timeOnSite, 0) AS time_on_site,
              totals.pageviews,
              # where the visitor came from
              trafficSource.source,
              trafficSource.medium,
              channelGrouping,
              # mobile or desktop
              device.deviceCategory,
              # geographic
              IFNULL(geoNetwork.country, "") AS country
          FROM `data-to-insights.ecommerce.web_analytics`,
             UNNEST(hits) AS h
            JOIN all_visitor_stats USING(fullvisitorid)
          WHERE 1=1
            # only predict for new visits
            AND totals.newVisits = 1
            AND date BETWEEN '20160801' AND '20170430' # train 9 months
          GROUP BY
          unique_session_id,
          will_buy_on_return_visit,
          bounces,
          time_on_site,
          totals.pageviews,
          trafficSource.source,
          trafficSource.medium,
          channelGrouping,
          device.deviceCategory,
          country
        );


# 2. Evaluate the model
#standardSQL
SELECT
  roc_auc,
  CASE
    WHEN roc_auc > .9 THEN 'good'
    WHEN roc_auc > .8 THEN 'fair'
    WHEN roc_auc > .7 THEN 'not great'
  ELSE 'poor' END AS model_quality
FROM
  ML.EVALUATE(MODEL ecommerce.classification_model_3,  (
WITH all_visitor_stats AS (
SELECT
  fullvisitorid,
  IF(COUNTIF(totals.transactions > 0 AND totals.newVisits IS NULL) > 0, 1, 0) AS will_buy_on_return_visit
  FROM `data-to-insights.ecommerce.web_analytics`
  GROUP BY fullvisitorid
)
# add in new features
SELECT * EXCEPT(unique_session_id) FROM (
  SELECT
      CONCAT(fullvisitorid, CAST(visitId AS STRING)) AS unique_session_id,
      # labels
      will_buy_on_return_visit,
      MAX(CAST(h.eCommerceAction.action_type AS INT64)) AS latest_ecommerce_progress,
      # behavior on the site
      IFNULL(totals.bounces, 0) AS bounces,
      IFNULL(totals.timeOnSite, 0) AS time_on_site,
      totals.pageviews,
      # where the visitor came from
      trafficSource.source,
      trafficSource.medium,
      channelGrouping,
      # mobile or desktop
      device.deviceCategory,
      # geographic
      IFNULL(geoNetwork.country, "") AS country
  FROM `data-to-insights.ecommerce.web_analytics`,
     UNNEST(hits) AS h
    JOIN all_visitor_stats USING(fullvisitorid)
  WHERE 1=1
    # only predict for new visits
    AND totals.newVisits = 1
    AND date BETWEEN '20170501' AND '20170630' # eval 2 months
  GROUP BY
  unique_session_id,
  will_buy_on_return_visit,
  bounces,
  time_on_site,
  totals.pageviews,
  trafficSource.source,
  trafficSource.medium,
  channelGrouping,
  device.deviceCategory,
  country
)
));


# 3. Make Predictions
SELECT
*
FROM
  ml.PREDICT(MODEL `ecommerce.classification_model_3`,
   (
WITH all_visitor_stats AS (
SELECT
  fullvisitorid,
  IF(COUNTIF(totals.transactions > 0 AND totals.newVisits IS NULL) > 0, 1, 0) AS will_buy_on_return_visit
  FROM `data-to-insights.ecommerce.web_analytics`
  GROUP BY fullvisitorid
)
  SELECT
      CONCAT(fullvisitorid, '-',CAST(visitId AS STRING)) AS unique_session_id,
      # labels
      will_buy_on_return_visit,
      MAX(CAST(h.eCommerceAction.action_type AS INT64)) AS latest_ecommerce_progress,
      # behavior on the site
      IFNULL(totals.bounces, 0) AS bounces,
      IFNULL(totals.timeOnSite, 0) AS time_on_site,
      totals.pageviews,
      # where the visitor came from
      trafficSource.source,
      trafficSource.medium,
      channelGrouping,
      # mobile or desktop
      device.deviceCategory,
      # geographic
      IFNULL(geoNetwork.country, "") AS country
  FROM `data-to-insights.ecommerce.web_analytics`,
     UNNEST(hits) AS h
    JOIN all_visitor_stats USING(fullvisitorid)
  WHERE
    # only predict for new visits
    totals.newVisits = 1
    AND date BETWEEN '20170701' AND '20170801' # test 1 month
  GROUP BY
  unique_session_id,
  will_buy_on_return_visit,
  bounces,
  time_on_site,
  totals.pageviews,
  trafficSource.source,
  trafficSource.medium,
  channelGrouping,
  device.deviceCategory,
  country
)
)
ORDER BY
  predicted_will_buy_on_return_visit DESC;
```
..


----
### Lab: [Set Up Network and HTTP Load Balancers](https://www.cloudskillsboost.google/focuses/12007?parent=catalog)
__

Sat, 19 Mar 2022

- Tasks:
    - Task 1: Set the default region and zone for all resources
    - Task 2: Create multiple web server instances
    - Task 3: Configure the load balancing service
    - Task 4: Sending traffic to your instances
    - Task 5: Create an HTTP load balancer
    - Task 6: Testing traffic sent to your instances

- Notes:
    - The gcloud create instance is quite powerful, allowing arbitrary
      startup scripts e.g. installing apache and customizing the default
      home page
    - `gcloud compute ... create` commands:
        - addresses: Static, public IP address
        - http-health-checks: Health check resource (Legacy)
        - target-pools: Targets for IP Forwarding
        - forwarding-rules: IP Forwarding rule
        - firewall rules: Firewall rule
        - instance-templates: Templates for instances
        - instance-groups managed: Managed instance groups
        - health-checks: Health check resource
        - backend-services: A grouping of instances like target pools
        - url-maps: URL rules to map requests to backend services. This
          is a load balancer!
        - target-http-proxies: HTTP proxy to route requests to url map
    - The step in Task 3 differs from Task 5:
        - Task 3: Load balancing results from using a combination of
          forwarding rules and target pool
        - Task 5: Load balancing results from grouping the instances in
          a `backend service`
        - The target-pool + forwarding-rule in Task 3 maps to
          backend-service + url-map + target-http-proxy in Task 5. One
          more level of indirection
    - Task 5 takes awhile to come up to wait a bit before clicking on
      the `Check my progress` button
    - [YouTube video](https://youtu.be/1ZW59HrRUzw)

- Task 1: Set the default region and zone for all resources
    ```
    gcloud config set compute/zone us-central1-a
    gcloud config set compute/region us-central1
    ```

- Task 2: Create multiple web server instances
    - The `tags` attribute is used to group resources
    - Below, the `network-lb-tag` is common to all three instances
    ```
    # Create web server "www1"
    gcloud compute instances create www1 \
      --image-family debian-9 \
      --image-project debian-cloud \
      --zone us-central1-a \
      --tags network-lb-tag \
      --metadata startup-script="#! /bin/bash
        sudo apt-get update
        sudo apt-get install apache2 -y
        sudo service apache2 restart
        echo '<!doctype html><html><body><h1>www1</h1></body></html>' | tee /var/www/html/index.html"

    # Create web server "www2"
    gcloud compute instances create www2 \
      --image-family debian-9 \
      --image-project debian-cloud \
      --zone us-central1-a \
      --tags network-lb-tag \
      --metadata startup-script="#! /bin/bash
        sudo apt-get update
        sudo apt-get install apache2 -y
        sudo service apache2 restart
        echo '<!doctype html><html><body><h1>www2</h1></body></html>' | tee /var/www/html/index.html"

    # Create web server "www3"
    gcloud compute instances create www3 \
      --image-family debian-9 \
      --image-project debian-cloud \
      --zone us-central1-a \
      --tags network-lb-tag \
      --metadata startup-script="#! /bin/bash
        sudo apt-get update
        sudo apt-get install apache2 -y
        sudo service apache2 restart
        echo '<!doctype html><html><body><h1>www3</h1></body></html>' | tee /var/www/html/index.html"

    # Create firewall rule:
    - Note the value of `target-tags`
    gcloud compute firewall-rules create www-firewall-network-lb \
        --target-tags network-lb-tag --allow tcp:80

    # List each instance to see external IP
    gcloud compute instances list

    # Run curl to check each instance
    curl http://35.224.73.148
    curl http://34.67.240.70
    curl http://35.184.186.55
    ```

- Task 3: Configure the load balancing service
    ```
    # Create a static, public IP for load balancer
    gcloud compute addresses create network-lb-ip-1 \
        --region us-central1

    # Create a legacy HTTP health check resource
    gcloud compute http-health-checks create basic-check

    # Create an empty target pool in the same region and use the
    #   health-check above.
    gcloud compute target-pools create www-pool \
        --region us-central1 --http-health-check basic-check

    # Add compute instances to the pool
    gcloud compute target-pools add-instances www-pool \
        --instances www1,www2,www3

    # Add a forwarding rule. Forwarding requests to the static, public
    # IP to the target pool containing the three web server instances
    gcloud compute forwarding-rules create www-rule \
        --region us-central1 \
        --ports 80 \
        --address network-lb-ip-1 \
        --target-pool www-pool
    ```

- Task 4: Sending traffic to your instances
    ```
    # View the forwarding rule
    gcloud compute forwarding-rules describe www-rule --region us-central1

    # Access the external IP address continuously
    while true; do curl -m1 34.68.108.146; done
    ```

- Task 5: Create an HTTP load balancer
    ```
    # 1. Create an instance template
    gcloud compute instance-templates create lb-backend-template \
       --region=us-central1 \
       --network=default \
       --subnet=default \
       --tags=allow-health-check \
       --image-family=debian-9 \
       --image-project=debian-cloud \
       --metadata=startup-script='#! /bin/bash
         apt-get update
         apt-get install apache2 -y
         a2ensite default-ssl
         a2enmod ssl
         vm_hostname="$(curl -H "Metadata-Flavor:Google" \
         http://169.254.169.254/computeMetadata/v1/instance/name)"
         echo "Page served from: $vm_hostname" | \
         tee /var/www/html/index.html
         systemctl restart apache2'

    # 2. Create a managed instance group using above template:
    gcloud compute instance-groups managed create lb-backend-group \
       --template=lb-backend-template --size=2 --zone=us-central1-a

    # 3. Create a firewall rule. The `target-tags` identify the affected instances
    gcloud compute firewall-rules create fw-allow-health-check \
        --network=default \
        --action=allow \
        --direction=ingress \
        --source-ranges=130.211.0.0/22,35.191.0.0/16 \
        --target-tags=allow-health-check \
        --rules=tcp:80

    # 4a. Create a static, external IP address
    gcloud compute addresses create lb-ipv4-1 \
        --ip-version=IPV4 \
        --global

    # 4b. Note the IP address reserved
    gcloud compute addresses describe lb-ipv4-1 \
        --format="get(address)" \
        --global
        # 34.120.130.89 (xxx)

    # 5. Create a health check for the load balancer
    gcloud compute health-checks create http http-basic-check \
        --port 80

    # 6. Create a backend service
    gcloud compute backend-services create web-backend-service \
        --protocol=HTTP \
        --port-name=http \
        --health-checks=http-basic-check \
        --global

    # 7. Add instance group to backend service
    gcloud compute backend-services add-backend web-backend-service \
        --instance-group=lb-backend-group \
        --instance-group-zone=us-central1-a \
        --global

    # 8. Create a URL map to send requests to backend service. This is
    #    the load balancer!
    gcloud compute url-maps create web-map-http \
        --default-service web-backend-service

    # 9. Create a target HTTP proxy to route requests to URL map
    gcloud compute target-http-proxies create http-lb-proxy \
        --url-map web-map-http

    # 10. Create forwarding rule to send requests to proxy
    gcloud compute forwarding-rules create http-content-rule \
        --address=lb-ipv4-1\
        --global \
        --target-http-proxy=http-lb-proxy \
        --ports=80
    ```

- Task 6: Testing traffic sent to your instances


..

----
### Lab: [Kubernetes Engine: Qwik Start](https://www.cloudskillsboost.google/focuses/878?parent=catalog)
__

Sat, 19 Mar 2022

- Tasks
    - Task 1: Set a default compute zone
    - Task 2: Create a GKE cluster
    - Task 3: Get authentication credentials for the cluster

- Notes:
    - Creating and deleting the cluster takes awhile so best to read
      through the lab in its entirety before starting the lab
    - [`Manage Containerized Apps with Kubernetes Engine`](https://youtu.be/u9nsngvmMK4)

- Task 1: Set a default compute zone
    ```
    # Set default zone
    gcloud config set compute/zone us-central1-a
    ```

- Task 2: Create a GKE cluster
    - A cluster has a cluster master and several workers called nodes
    - Note: Cluster names must start with a letter and end with an
      alphanumeric, and cannot be longer than 40 characters.
    ```
    # Create a cluster
    # gcloud container clusters create [CLUSTER-NAME]
    gcloud container clusters create my-cluster
    ```

- Task 3: Get authentication credentials for the cluster
    - Auth credentials needed to interact with cluster
    ```
    # Get cluster credentials
    # gcloud container clusters get-credentials [CLUSTER-NAME]
    gcloud container clusters get-credentials my-cluster
    ```

- Task 4: Deploy an application to the cluster
    - Lab deploys a hello-app
    - `Deployment` objects are for stateless applications like web servers
    - `Service` objects define rules and load balancing for accessing
      app from the net
    ```
    # Create a new Deployment of 'hello-server' from a pre-defined container image
    kubectl create deployment hello-server --image=gcr.io/google-samples/hello-app:1.0
    # Expect: deployment.apps/hello-server created

    # Create a Service, which exposes app to external traffic
    kubectl expose deployment hello-server --type=LoadBalancer --port 8080
    # Expect: service/hello-server exposed

    # Inspect service:
    kubectl get service
    # Expect: NAME  TYPE    CLUSTER-IP  EXTERNAL-IP    PORT(S)    AGE
    ```

- Task 5: Deleting the cluster
    ```
    # gcloud container clusters delete [CLUSTER-NAME]
    gcloud container clusters delete my-cluster

    ```
- Resources:
..

----
### Lab: [Create a Virtual Machine](https://www.cloudskillsboost.google/focuses/3563?parent=catalog)
__

Sat, 19 Mar 2022

- Tasks
    - Create a virtual machine with the Cloud Console.
    - Create a virtual machine with the gcloud command line.
    - Deploy a web server and connect it to a virtual machine.

- `gcloud`:
    ```
    # List active account name
    gcloud auth list

    # List project ID
    gcloud config list project
    ```

- Typical VM parameters:
    - <img src="img/notes-typical-vm-parameters.png" alt="" width="80%"/>

- Install NGINX web server on a fresh VM
    ```
    sudo su -
    apt-get update
    apt-get install nginx -y

    # Check that nginx is running
    ps auwx | grep nginx
    ```

- Create a VM via cloud shell:
    ```
    gcloud compute instances create gcelab2 --machine-type n1-standard-2 --zone us-central1-f

    # Get help on CLI
    gcloud compute instances create --help

    # Set default zone and region
    gcloud config set compute/zone ...
    gcloud config set compute/region ...

    # SSH connect
    gcloud compute ssh gcelab2 --zone us-central1-f
    ```
..


----
### Lab: [Getting Started with Cloud Shell and gcloud](https://www.cloudskillsboost.google/focuses/563?parent=catalog)
__

Sat, 19 Mar 2022

- Task 1: Configure your environment
    ```
    # See default region and zone
    gcloud config get-value compute/zone
    gcloud config get-value compute/region

    # Describe project
    # gcloud compute project-info describe --project <your_project_ID>
    gcloud compute project-info describe --project qwiklabs-gcp-02-b02d242a3d87

    # Set environment variables:
    # export PROJECT_ID=<your_project_ID>
    export PROJECT_ID=qwiklabs-gcp-02-b02d242a3d87

    # export ZONE=<your_zone>
    export ZONE=us-central1-a

    # Create a VM
    gcloud compute instances create gcelab2 --machine-type n1-standard-2 --zone $ZONE

    - gcloud compute allows you to manage your Compute Engine resources in a format that's simpler than the Compute Engine API.
    - instances create creates a new instance.
    - gcelab2 is the name of the VM.
    - The --machine-type flag specifies the machine type as n1-standard-2.
    - The --zone flag specifies where the VM is created.

    If you omit the --zone flag, the gcloud tool can infer your desired zone based on your default properties. Other required instance settings, such as machine type and image, are set to default values if not specified in the create command.

    # To get help
    gcloud compute instances create --help

    # gcloud help
    gcloud -h
    gcloud config --help

    # List gcloud config
    gcloud config list                  # common user config
    gcloud config list --all            # all config

    # List gcloud components
    gcloud components list
    ```

- Task 2: Install a new component
    ```
    # Install beta components:
    sudo apt-get install google-cloud-sdk

    # Enable gcloud interactive mode:
    gcloud beta interactive

    ```

- Task 3: Connect to VM via SSH
    ```
    gcloud compute ssh gcelab2 --zone $ZONE

    ```
..


----
### A Completion Badge vs A Skill Badge in Quests
__

- A completion badge is given upon completion

- A skill badge is given after an ability is tested

- <img src="img/notes-completion-vs-skill-badge.png" alt="" width="50%"/>
..


----
### Apache Beam and Google DataFlow
__

- How are they related?

- Apache Beam is the program, DataFlow is the engine

- If they were database technologies, Apache Beam would be SQL programs
  and DataFlow would be the database engine running those SQL programs

- There other pipeline runners other than Google DataFlow. But the key
  feature of DataFlow is autoscaling

- See also: https://medium.com/swlh/dataflow-and-apache-beam-the-result-of-a-learning-process-since-mapreduce-c591b2ab180e (TODO)
- Apache Beam video analytics for Football games (28:57m)
    - https://www.youtube.com/watch?v=dPuE30kY6-c
    - Design and create a platform that can handle auto-production and
      broadcasting, for up to hundreds of games per weekend with as
      little time delay as possible
..


----
### Google Cloud Offerings Group
__

- IAM & Admin (Security)
- APIs & Services
- Cloud Offerings e.g. Compute, Storage, Network, BigQuery etc..
..


----
### GSP001 - Creating a Virtual Machine
__

- Course Link:
    - https://www.cloudskillsboost.google/focuses/3563?parent=catalog
    - 40 minutes 1 Credit

- Goals
    - Create a VM on GCP
    - Create a VM using gcloud CLI
    - Deploy an NGinx web server

- Notes on Cloud Shell:
    - A VM loaded with development tools
    - 5GB persistence home directory on Google Cloud
    - In Cloud Console, click on `Activate Cloud Shell` button
    - <img src="img/GSP001_1-cloud-shell.png" alt="" width="80%"/>

- `gcloud`:
    - Docs: https://cloud.google.com/sdk/gcloud
    ```
    List active account names
        gcloud auth list

    List project ID
        gcloud config list project
    ```

- Regions and zones
    - One region has many zones
    - Naming convention: REGION-ZONE
        - Region: us-west1, Zone: a  -->  us-west1-a
        - Region: us-west1, Zone: b  -->  us-west1-b
        - Region: asia-east1, Zone: a  -->  asia-east1-a
        - etc
    - Zones are like data centers (I think)
    - Resources such as VM, Storage, etc live in zones aka data centers
..


----
### List of Google Cloud Quests
__

- BigQuery for Data Warehousing
    - https://www.cloudskillsboost.google/quests/68
    - Fundamental, 6 hours, 5 Credits

- BigQuery Basics for Data Analysts
    - https://www.cloudskillsboost.google/quests/69
    - Fundamental, 5 hours, 2 Credits

- Courses, Quests and Labs for Data Engineer Role:
    - https://www.cloudskillsboost.google/catalog?keywords=&locale=&solution%5B%5D=any&role%5B%5D=data-engineer&skill-badge%5B%5D=any&format%5B%5D=any&level%5B%5D=any&duration%5B%5D=any&language%5B%5D=any

..


----
### Google API Design Guide for Cloud APIs
__

- A good read to understand Google Cloud Design principles
    - https://cloud.google.com/apis/design/
..


----
### Google APIs Explorer
__

- A useful tool to try out Google Cloud APIs
    - https://developers.google.com/apis-explorer/#p/

- A lab to try APIs Explorer
    - https://google.qwiklabs.com/catalog_lab/1241
..


----
### Data Engineer Learning Path

__ Articles on online training

- https://solutionsreview.com/data-management/the-best-data-warehousing-courses-and-online-training/
..
__

- Link:
    - https://www.cloudskillsboost.google/paths/16

- Why take this path
    - To obtain the Google Cloud Data Engineer certification

- Costs:
    - https://www.cloudskillsboost.google/payments/new
    - Buy Advantage Subscription for USD 29 (~ 1000 THB) for a monthly
      subscription and get unlimited credits
    - Possible to get a 1-month free promotion by going to 
        - https://go.qwiklabs.com/qwiklabs-free
        - Click on `Accept the challenge!`
    - <img src="img/labs-accept-the-challenge.png" alt="" width="80%"/>

- Courses and Quests:
    - Estimated Total:
        - 14 Days / 299 Credits
    - A Tour of Google Cloud Hands-On Labs
        - https://www.cloudskillsboost.google/focuses/2794?parent=catalog
        - 45 mins / Free
    - Google Cloud Big Data and Machine Learning Fundamentals
        - https://www.cloudskillsboost.google/course_templates/3
        - 1 day / Introductory / 20 Credits
    - Modernizing Data Lakes and Data Warehouses with Google Cloud
        - https://www.cloudskillsboost.google/course_templates/54
        - 1 day / Advanced / 20 Credits
    - Building Batch Data Pipelines on Google Cloud
        - https://www.cloudskillsboost.google/course_templates/53
        - 1 day / Advanced / 45 Credits
    - Building Resilient Streaming Analytics Systems on Google Cloud
        - https://www.cloudskillsboost.google/course_templates/52
        - 1 day / Advanced / 30 Credits
    - Smart Analytics, Machine Learning, and AI on Google Cloud
        - https://www.cloudskillsboost.google/course_templates/55
        - 1 day / Advanced / 25 Credits
    - Serverless Data Processing with Dataflow: Foundations
        - https://www.cloudskillsboost.google/course_templates/218
        - Advanced / 5 Credits
    - Serverless Data Processing with Dataflow: Develop Pipelines
        - https://www.cloudskillsboost.google/course_templates/229
        - Expert / 70 Credits
    - Serverless Data Processing with Dataflow: Operations
        - https://www.cloudskillsboost.google/course_templates/264
        - Expert / 30 Credits
    - Create and Manage Cloud Resources
        - https://www.cloudskillsboost.google/quests/120
        - 5 hours / Introductory / 6 Credits
    - Perform Foundational Data, ML, and AI Tasks in Google Cloud
        - https://www.cloudskillsboost.google/quests/117
        - 6 hours / Introductory / 16 Credits
    - Engineer Data in Google Cloud
        - https://www.cloudskillsboost.google/quests/132
        - 6 hours / Advanced / 27 Credits
    - Preparing for the Google Cloud Professional Data Engineer Exam
        - Expert / 10 Credits

- Other Relevant Courses and Quests:
    - Google Cloud Essentials
        - https://www.cloudskillsboost.google/quests/23
        - 4 hours / Introductory / 5 Credits
..

__ Microsoft Certifications

- Azure Data Fundamentals DP-900 [skillport](https://acm.skillport.com/skillportfe/main.action?path=summary/BOOKS/157587#browse/12aea111-8721-4ba8-9ceb-c8d382043faf)

- Azure Data Engineer Associate [DP-203] [skillport](https://acm.skillport.com/skillportfe/main.action?path=summary/BOOKS/157587#browse/425ca390-aa45-4de4-b9cd-f746020f076a)

- MCSE: Data Management and Analytics [skillport](https://acm.skillport.com/skillportfe/main.action?path=summary/BOOKS/157587#browse/fd1727c3-3cf1-4b18-9583-7ca5eb852f3b)

- SSIS Design Patterns: https://www.pluralsight.com/courses/ssis-design-patterns-data-warehousing

..

__ AWS

- Data Engineer nano degree
    - https://www.udacity.com/course/data-engineer-nanodegree--nd027
..

__ Databricks Training
..

__ Google

- Google Data Engineer [skillport](https://acm.skillport.com/skillportfe/main.action?path=summary/BOOKS/157587#browse/7bcdd429-959d-4f02-baf8-78edb874b6b5)

..

__ Coursera

- Design and Build a Data Warehouse for Business Intelligence Implementation
    - https://www.coursera.org/learn/data-warehouse-bi-building
- Relational Database Support for Data Warehouses
    - https://www.coursera.org/learn/dwrelational
- Design and Build a Data Warehouse for Business Intelligence Implementation
    - https://www.coursera.org/learn/data-warehouse-bi-building
- Data Warehouse Concepts, Design, and Data Integration
    - https://www.coursera.org/learn/dwdesign
- Data Warehousing for Business Intelligence Specialization
    - https://www.coursera.org/specializations/data-warehousing
..

__ Teradata

- Citizen Data Scientist
    - https://www.teradata.com/University/Citizen-Data-Scientist

- Teradata Certification
    - https://www.teradata.com/University/Certification

..

__ Python

- Faust: Getting Started with Stream Processing
    - https://acm.skillport.com/skillportfe/main.action#summary/COURSES/CDE$110302:_ss_cca:it_pyspwfdj_01_enus

..


----
### Unsorted notes from Azure DP-900
__

Notions of:
    - Data Workloads
    - Workload SLAs
    - Workload Classification, Importance and Isolation

- A Graph Database
    - Could be useful for dependency graphs
..

